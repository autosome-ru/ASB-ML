{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-hsr0Me3ynD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS5Ez_JYJjKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import sklearn\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from itertools import compress\n",
        "from sklearn.utils import shuffle \n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, \\\n",
        "accuracy_score, roc_curve, precision_recall_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49VfsusyMikH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ASBException(Exception):\n",
        "    pass\n",
        "\n",
        "class ASBIndex:\n",
        "    \"\"\"\n",
        "    Object to work with the information stored in the database\n",
        "    \n",
        "    Replacement for the hdf to be able to use 7z zipping\n",
        "    \"\"\"\n",
        "    \n",
        "    SPECIFIC_FEATURES = {'motif_log_pref', 'motif_log_palt', 'motif_fc', 'motif_pos', 'is_asb'}\n",
        "\n",
        "    def __init__(self, base_ids, base_dir, specific_features=None):\n",
        "        \"\"\"\n",
        "        Initialize object. \n",
        "        Should not be run directly. Objects of ASBIndex must be create through class methods\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        base_ids: list\n",
        "            SNV ids in the database\n",
        "        base_dir: str\n",
        "            base directory with SNV features. Each feature is stored in the vector with each element corresponding to snv in base_ids\n",
        "        specific_features: set[str] - set of dataset-specific features\n",
        "        \"\"\"\n",
        "        self.base_ids = base_ids\n",
        "        self.base_dir = base_dir\n",
        "        \n",
        "        specific_features = specific_features or ASBIndex.SPECIFIC_FEATURES\n",
        "        self.specific_features = specific_features\n",
        "\n",
        "    def retrieve_datatable(self, \n",
        "                           query_ids, \n",
        "                           query_feat, \n",
        "                           dataset_name=None):\n",
        "        \"\"\" retrieve infromation about SNV ids\n",
        "        Parameters\n",
        "        ----------\n",
        "        query_ids: list[str] - SNV ids to get information about\n",
        "        query_feat: list[str] - features to retrieve\n",
        "        dataset_name: str - name of the dataset to retrieve. Required if any dataset-specific features are queried\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            Table containing information about quered ids\n",
        "        \"\"\"\n",
        "        \n",
        "        q_set = set(query_ids)\n",
        "        mask = np.array([(q in q_set) for q in self.base_ids])\n",
        "        \n",
        "        if mask.sum() != len(query_ids):\n",
        "            not_found = q_set - set(self.base_ids)\n",
        "            if not_found:\n",
        "                raise ASBException(f\"Ids weren't found: {', '.join(q_set - set(self.base_ids))}\")\n",
        "            else:\n",
        "                k, c = np.unique(query_ids, return_counts=True)\n",
        "                k = k[c > 1]\n",
        "                if k.shape[0] > 0:\n",
        "                    raise ASBException(f\"Duplicated ids: {', '.join(list(k))}\")\n",
        "                else:\n",
        "                    raise ASBException(\"Uknown error occured\")\n",
        "                \n",
        "                \n",
        "        order = {k : i for i, k in enumerate(query_ids)}\n",
        "        dt = {}\n",
        "    \n",
        "        for col in query_feat:\n",
        "            try:\n",
        "                col_path = self.get_feat_path(col, dataset_name)\n",
        "                col_ar = np.load(col_path, allow_pickle=True)\n",
        "            except FileNotFoundError:\n",
        "                if col in self.specific_features:\n",
        "                    print(f\"File corresponding to dataset-specific feature '{col}' from dataset '{dataset_name}' doesn't exist\", \n",
        "                      file=sys.stderr)\n",
        "                else:\n",
        "                    print(f\"File corresponding to feature '{col}' doesn't exist\", file=sys.stderr)\n",
        "            else:\n",
        "                dt[col] = col_ar[mask]\n",
        "\n",
        "        dt =  pd.DataFrame(dt, columns=dt.keys())\n",
        "        dt['ID'] = list(compress(self.base_ids, mask))\n",
        "        dt['ID_mapped'] = dt['ID'].apply(lambda x: order[x])   # to run on Python3.6\n",
        "        dt.sort_values(by=\"ID_mapped\", axis=0, inplace=True)\n",
        "        dt.drop(\"ID_mapped\", axis=1, inplace=True)\n",
        "        dt.set_index(\"ID\", inplace=True)\n",
        "        \n",
        "        return dt\n",
        "    \n",
        "    \n",
        "    def add_columns_from_tab(self, \n",
        "                             tab,\n",
        "                             dataset_name=None,\n",
        "                             ID_name=\"index\"):\n",
        "        \"\"\"\n",
        "        Add features from the datatable to the database\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        tab: pd.DataFrame - table containing features to add\n",
        "        dataset_name: str -  name of the dataset. Required if any dataset-specific features are added\n",
        "        ID_name: str - name of column containing ids, if ID_name == \"index\" table index will be used\n",
        "        \"\"\"\n",
        "        if ID_name != \"index\":\n",
        "            tab = tab.set_index(ID_name)\n",
        "        \n",
        "        \n",
        "        for col in tab.columns:\n",
        "            path = self.get_feat_path(col, dataset_name)\n",
        "            if os.path.exists(path):\n",
        "                if col in self.specific_features:\n",
        "                    raise ASBException(f\"Dataset specific feature {col} for '{dataset_name}' dataset already exists\"\n",
        "                                       f\", specify other `dataset_name` to add it to the table\")\n",
        "                else:\n",
        "                    raise ASBException(f\"Feature {col} already exists\")\n",
        "        \n",
        "        ext_tab = tab.reindex(self.base_ids)\n",
        "        \n",
        "        for col in ext_tab.columns:\n",
        "            out = self.get_feat_path(col, dataset_name)\n",
        "            np.save(out, ext_tab[col])\n",
        "            \n",
        "    def get_index_path(self):\n",
        "        \"\"\"\n",
        "        return path to the index file of the database\n",
        "        \"\"\"\n",
        "        return os.path.join(self.base_dir, \"id.lst\")\n",
        "    \n",
        "    def get_feat_path(self, feat, dataset_name=None):\n",
        "        \"\"\"\n",
        "        return path to the feature in the database\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        feat: str - name of the feature\n",
        "        dataset_name: str - name of the dataset. Required if feature is dataset-specific\n",
        "        \"\"\"\n",
        "        \n",
        "        if feat in self.specific_features:\n",
        "            if dataset_name is None:\n",
        "                raise ASBException(f\"Dataset name is required to get path for dataset-specific feature {feat}\")\n",
        "            feat_name = f\"{feat}_{dataset_name}\"\n",
        "        else:\n",
        "            feat_name = feat\n",
        "        return os.path.join(self.base_dir, f\"{feat_name}.npy\")\n",
        "    \n",
        "    def get_specific_features_path(self):\n",
        "        \"\"\"\n",
        "        return path to the file with list of dataset-specific features\n",
        "        \"\"\"\n",
        "        return os.path.join(self.base_dir, \"specific_features.lst\")\n",
        "    \n",
        "    \n",
        "    def get_subset_ids_dir(self):\n",
        "        \"\"\"\n",
        "        return path to directory storing ids subsets\n",
        "        \"\"\"\n",
        "        \n",
        "        return os.path.join(self.base_dir, \"ids\")\n",
        "    \n",
        "    def get_subset_feats_dir(self):\n",
        "        \"\"\"\n",
        "        return path to directory storing features subsets\n",
        "        \"\"\"\n",
        "        \n",
        "        return os.path.join(self.base_dir, \"feats\")\n",
        "    \n",
        "    def get_ids_subset_path(self, name):\n",
        "        \"\"\"\n",
        "        return path to the file with ids subset\n",
        "        \n",
        "        Paramters\n",
        "        ---------\n",
        "        name - subset name\n",
        "        \"\"\"\n",
        "        \n",
        "        return os.path.join(self.get_subset_ids_dir(), f\"{name}.lst\")\n",
        "    \n",
        "    \n",
        "    def get_feats_subset_path(self, name):\n",
        "        \"\"\"\n",
        "        return path to the file with features subset \n",
        "        \n",
        "        Paramters\n",
        "        ---------\n",
        "        name - subset name\n",
        "        \"\"\"\n",
        "        \n",
        "        return os.path.join(self.get_subset_feats_dir(), f\"{name}.lst\")\n",
        "    \n",
        "    \n",
        "    def write_index(self):\n",
        "        \"\"\"\n",
        "        write database index file\n",
        "        \"\"\"\n",
        "        with open(self.get_index_path(), \"w\") as out:\n",
        "            out.write(\"\\n\".join(self.base_ids))\n",
        "            \n",
        "    def write_specific_features(self):\n",
        "        \"\"\"\n",
        "        write dataset-specific features file\n",
        "        \"\"\"\n",
        "        with open(self.get_specific_features_path(), \"w\") as out:\n",
        "            out.write(\"\\n\".join(self.specific_features))\n",
        "    \n",
        "    \n",
        "    def make_basedir(self):\n",
        "        \"\"\"\n",
        "        make database basedir\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.base_dir):\n",
        "            raise ASBException(f\"{self.base_dir} already exists\")\n",
        "        os.makedirs(self.base_dir)\n",
        "        ids_path = os.path.join(self.base_dir, \"ids\")\n",
        "        os.makedirs(self.get_subset_ids_dir())\n",
        "        os.makedirs(self.get_subset_feats_dir())\n",
        "            \n",
        "            \n",
        "    @classmethod\n",
        "    def create(cls, base_ids, base_dir, specific_features=None):\n",
        "        \"\"\"\n",
        "        Create empty database to store features of `base_ids`\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        base_ids: list\n",
        "            SNV ids in the database\n",
        "        base_dir: str\n",
        "            base directory with SNV features. \n",
        "            Each feature is stored in the vector with each element corresponding to snv in base_ids\n",
        "        specific_features: set[str] \n",
        "            set of dataset-specific features\n",
        "        \"\"\"\n",
        "        specific_features = specific_features or ASBIndex.SPECIFIC_FEATURES\n",
        "        self = cls(base_ids, base_dir, specific_features.copy())\n",
        "        self.make_basedir()\n",
        "        self.write_index()\n",
        "        self.write_specific_features()\n",
        "        return self\n",
        "        \n",
        "                \n",
        "    @classmethod\n",
        "    def from_table(cls, \n",
        "                   tab,\n",
        "                   base_dir, \n",
        "                   dataset_name=None,\n",
        "                   specific_features=None,\n",
        "                   ID_name=\"index\"):\n",
        "        \"\"\"\n",
        "        Create database from a datatable\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        tab: pd.DataFrame - table to create database from \n",
        "        basedir: str - path to database base directory\n",
        "        dataset_name: str - name of the dataset, required if any feature in the table is dataset-specific\n",
        "        ID_name: str - name of ids column\n",
        "        \"\"\" \n",
        "        \n",
        "        specific_features = specific_features or ASBIndex.SPECIFIC_FEATURES\n",
        "        if ID_name != \"index\":\n",
        "            base_ids = list(map(str, tab[ID_name]))\n",
        "        else:\n",
        "            base_ids = list(map(str, tab.index))\n",
        "    \n",
        "        self = ASBIndex.create(base_ids, base_dir, specific_features.copy())\n",
        "\n",
        "        for col in tab.columns:\n",
        "            out = self.get_feat_path(col, dataset_name)\n",
        "            np.save(out, tab[col])\n",
        "            \n",
        "        return self\n",
        "    \n",
        "    @classmethod\n",
        "    def load(cls, base_dir):\n",
        "        \"\"\"\n",
        "        Load  database from dir\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        basedir: str - path to database base directory\n",
        "        \"\"\"\n",
        "        self = cls([], base_dir)\n",
        "        \n",
        "        with open(self.get_index_path(), \"r\") as inp:\n",
        "            base_ids = [line.strip() for line in inp]\n",
        "            \n",
        "        with open(self.get_specific_features_path(), \"r\") as inp:\n",
        "            specific_features = set(line.strip() for line in inp)\n",
        "        \n",
        "        self.base_ids = base_ids\n",
        "        self.specific_features = specific_features\n",
        "        return self\n",
        "    \n",
        "    def define_ids_subset(self, ids, name):\n",
        "        ids_path = self.get_ids_subset_path(name)\n",
        "        if os.path.exists(ids_path):\n",
        "            raise ASBException(f\"ids subset with name '{name}' already exists\")\n",
        "        with open(ids_path, \"w\") as out:\n",
        "            out.write(\"\\n\".join(ids))\n",
        "            \n",
        "    def load_ids_subset(self, name):\n",
        "        ids_path = self.get_ids_subset_path(name)\n",
        "        if not os.path.exists(ids_path):\n",
        "            raise ASBException(f\"ids subset with name '{name}' doesn't exist\")\n",
        "        with open(ids_path, \"r\") as inp:\n",
        "            ids = [line.strip() for line in inp]\n",
        "        return ids \n",
        "    \n",
        "    def define_feat_subset(self, feats, name):\n",
        "        feats_path = self.get_feats_subset_path(name)\n",
        "        if os.path.exists(feats_path):\n",
        "            raise ASBException(f\"features subset with name '{name}' already exists\")\n",
        "        with open(feats_path, \"w\") as out:\n",
        "            out.write(\"\\n\".join(feats))\n",
        "\n",
        "    def load_feat_subset(self, name):\n",
        "        feats_path = self.get_feats_subset_path(name)\n",
        "        if not os.path.exists(feats_path):\n",
        "            raise ASBException(f\"features subset with name '{name}' doesn't exist\")\n",
        "        with open(feats_path, \"r\") as inp:\n",
        "            feats = [line.strip() for line in inp]\n",
        "        return feats\n",
        "        \n",
        "                                    \n",
        "    def add_ids(self):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def add_specific_feature(self):\n",
        "        raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGmXp5NoPjQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_shuffle(df, \n",
        "               copy=True,\n",
        "               random_state=1):\n",
        "    '''\n",
        "    shuffle values in dataframe columnwise\n",
        "     \n",
        "    Parameters \n",
        "    ----------\n",
        "    df: pd.DataFrame - dataframe\n",
        "    copy: bool - copy dataframe or do shuffle inplace\n",
        "    random_state: int \n",
        "    '''\n",
        "    if copy:\n",
        "        df = df.copy()\n",
        "    for column in df.columns:\n",
        "        df.loc[:, column] = shuffle(df[[column]].values, random_state=random_state)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2y6IBkl4gc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_feature_importance(clf,\n",
        "                              df, \n",
        "                              label_col=\"is_asb\", \n",
        "                              shuffle_features=True, \n",
        "                              random_state=1):\n",
        "    '''\n",
        "    get random feature importance (for shuffled input features)\n",
        "    Parameters\n",
        "    ----------\n",
        "    clf: classifier - classifier with sklearn-friendly interface  \n",
        "    df: pd.DataFrame - dataframe for training  \n",
        "    label_col: - labels column name\n",
        "    shuffle_features: bool - shuffle features or shuffle labels\n",
        "    random_state: int\n",
        "    '''\n",
        "\n",
        "    X = df.drop(columns=[label_col])\n",
        "    y = df[label_col]\n",
        "\n",
        "    if shuffle_features:\n",
        "        X = df_shuffle(X, copy=False, random_state=random_state)\n",
        "    else:\n",
        "        y = shuffle(df[label_col], random_state=random_state)\n",
        "    \n",
        "    clf.fit(X, y)\n",
        "    max_rnd_importance = np.max(clf.feature_importances_)\n",
        "    return max_rnd_importance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr-kOlkrczqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_table(df, filepath):\n",
        "    '''\n",
        "    Write pandas dataframe in tab-separated format\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: pd.DataFrame - dataframe\n",
        "    filepath: str - path to write\n",
        "    '''\n",
        "    df.to_csv(filepath, sep=\"\\t\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8isYUmx8U3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def curves(y_true, pred_prob):\n",
        "    '''\n",
        "    calculate points for rocauc and prauc curves\n",
        "    Parameters \n",
        "    ----------\n",
        "    y_true: np.array - vector of true labels \n",
        "    pred_prob: np.array - vector of probabilities for class 1\n",
        "    '''\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, pred_prob)\n",
        "    pr, rec, thresh_prauc = precision_recall_curve(y_true, pred_prob)\n",
        "    df_roc = pd.DataFrame(data={'fpr': fpr, \n",
        "                                'tpr': tpr, \n",
        "                                'thresholds_roc': thresholds})\n",
        "    df_pr = pd.DataFrame(data={'precision': pr[:-1], \n",
        "                               'recall': rec[:-1], \n",
        "                               'thresholds_prauc': thresh_prauc})\n",
        "    return df_roc, df_pr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZNHEfE0LKG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation_chromosomes(clf,\n",
        "              data, \n",
        "              importance_threshold,\n",
        "              ids_subsets=None, \n",
        "              chr_label=\"chr\",\n",
        "              y_label=\"is_asb\"):\n",
        "    '''\n",
        "    clf: classifer\n",
        "    data: pd.DataFrame - dataset for chromosomewise crossvalidation\n",
        "    importance_threshold: float - threshold to determine important features\n",
        "    ids_subsets: Union[dict, None] - subsets to calculate validations scores on.\n",
        "    chr_label: str - name of chromosome column in the dataset\n",
        "    y_label: str - name of label column in the dataset\n",
        "    '''\n",
        "    df_roc_lst = []\n",
        "    df_pr_lst = []\n",
        "    dt = {\"val_chr\": [], \n",
        "          \"roc_auc\": [], \n",
        "          \"f1\": [], \n",
        "          \"accuracy\": [],\n",
        "          \"pr_auc\": [], \n",
        "          \"important_features\": [],\n",
        "          \"feature_scores\": [],\n",
        "          \"subset_name\": []}\n",
        "\n",
        "    if ids_subsets is None:\n",
        "        ids_subsets = {}\n",
        "\n",
        "    ids_subsets['all'] = None\n",
        "\n",
        "    for i in range(1, 23):\n",
        "        val_chr = f\"chr{i}\"\n",
        "        \n",
        "        df_train = data[data[chr_label] != val_chr]\n",
        "        df_val = data[data[chr_label] == val_chr]\n",
        "        if (df_train.shape[0] <= 1 or df_val.shape[0] <= 1): # no data for chromosome i or only one class\n",
        "            print(f\"Skipping validation for chromosome {val_chr} - not enought data\")\n",
        "            continue\n",
        "        \n",
        "        X_train = df_train.drop(columns = [y_label, chr_label])\n",
        "        X_val = df_val.drop(columns = [y_label, chr_label])\n",
        "        y_train = df_train[y_label]\n",
        "        y_val = df_val[y_label]\n",
        "        if len(np.unique(y_train)) == 1 or len(np.unique(y_val)) == 1: # only one class for chromosome i\n",
        "            print(f\"Skipping validation for chromosome {val_chr} - not enought data\")\n",
        "            continue\n",
        "\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        for subset_name, ids in ids_subsets.items():\n",
        "            dt['val_chr'].append(val_chr)\n",
        "            dt['subset_name'].append(subset_name)\n",
        "\n",
        "            if subset_name != \"all\":\n",
        "                X_val_part = X_val.reindex(ids).dropna()\n",
        "                y_val_part = y_val.reindex(ids).dropna()\n",
        "            else:\n",
        "                X_val_part = X_val\n",
        "                y_val_part = y_val\n",
        "          \n",
        "            if len(np.unique(y_val_part)) < 2:\n",
        "                print(f\"Skipping validation for chromosome {val_chr} on subset {subset_name}- not enought data\",\n",
        "                      file=sys.stderr)\n",
        "                dt['roc_auc'].append(None)\n",
        "                dt['f1'].append(None)\n",
        "                dt['accuracy'].append(None)\n",
        "                dt['pr_auc'].append(None)\n",
        "                dt['important_features'].append(None)\n",
        "                dt['feature_scores'].append(None)\n",
        "                continue\n",
        "\n",
        "            \n",
        "\n",
        "            rfpred = clf.predict_proba(X_val_part)[:, 1]\n",
        "            pred_classes = clf.predict(X_val_part)\n",
        "            dt['roc_auc'].append(roc_auc_score(y_val_part, rfpred))\n",
        "            dt['f1'].append(f1_score(y_val_part, pred_classes))\n",
        "            dt['accuracy'].append(accuracy_score(y_val_part, pred_classes))\n",
        "            dt['pr_auc'].append(average_precision_score(y_val_part, rfpred)) \n",
        "            \n",
        "            imp_mask = clf.feature_importances_ > importance_threshold\n",
        "            dt['important_features'].append(list(X_train.columns[imp_mask]))\n",
        "            dt['feature_scores'].append(clf.feature_importances_[imp_mask])\n",
        "            \n",
        "            df_roc_part, df_pr_part = curves(y_val_part, rfpred)\n",
        "            df_roc_part['val_chr'] = val_chr\n",
        "            df_roc_part['subset_name'] = subset_name\n",
        "            df_pr_part['val_chr'] = val_chr\n",
        "            df_pr_part['subset_name'] = subset_name\n",
        "            df_roc_lst.append(df_roc_part)\n",
        "            df_pr_lst.append(df_pr_part)\n",
        "\n",
        "\n",
        "    df_qual = pd.DataFrame(dt, columns=dt.keys())\n",
        "    df_roc = pd.concat(df_roc_lst)\n",
        "    df_pr = pd.concat(df_pr_lst)\n",
        "\n",
        "    return df_qual, df_roc, df_pr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y02X9DAHECOv",
        "colab_type": "text"
      },
      "source": [
        "# Downloading data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlSK6R5Bld4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "7e6acc70-0a51-4e21-d1c9-43e7ff628ea4"
      },
      "source": [
        "%%bash\n",
        "pip3 install wldhx.yadisk-direct\n",
        "wget -O asb_data.7z $(yadisk-direct https://yadi.sk/d/u61gSHWT1wHkxw)\n",
        "7za x asb_data.7z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wldhx.yadisk-direct in /usr/local/lib/python3.6/dist-packages (0.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from wldhx.yadisk-direct) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->wldhx.yadisk-direct) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->wldhx.yadisk-direct) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->wldhx.yadisk-direct) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->wldhx.yadisk-direct) (3.0.4)\n",
            "\n",
            "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.30GHz (306F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "1 file, 2849142134 bytes (2718 MiB)\n",
            "\n",
            "Extracting archive: asb_data.7z\n",
            "--\n",
            "Path = asb_data.7z\n",
            "Type = 7z\n",
            "Physical Size = 2849142134\n",
            "Headers Size = 28817\n",
            "Method = LZMA2:24\n",
            "Solid = +\n",
            "Blocks = 7\n",
            "\n",
            "Everything is Ok\n",
            "\n",
            "Folders: 3\n",
            "Files: 4697\n",
            "Size:       8654630213\n",
            "Compressed: 2849142134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DA_wPukFbOy",
        "colab_type": "text"
      },
      "source": [
        "# Global vars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2LDD-CzEy-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_LINE_NAMES = [\"foreskin_keratinocyte\", \n",
        "                   \"HepG2__hepatoblastoma_\",\n",
        "                   \"LNCaP__prostate_carcinoma_\",\n",
        "                   \"K562__myelogenous_leukemia_\",\n",
        "                   \"A549__lung_carcinoma_\",\n",
        "                   \"GM12878__female_B-cells_\",\n",
        "                   \"VCaP__prostate_carcinoma_\",\n",
        "                   \"CD14+_monocytes\",\n",
        "                   \"HEK293__embryonic_kidney_\",\n",
        "                   \"MCF7__Invasive_ductal_breast_carcinoma_\"]\n",
        "TF_NAMES = [\"SPI1\",\n",
        "            \"CTCF\",\n",
        "            \"FOXK2\",\n",
        "            \"STAT1\",\n",
        "            \"FOXA1\",\n",
        "            \"IKZF1\",\n",
        "            \"ESR1\",\n",
        "            \"RAD21\",\n",
        "            \"ANDR\",\n",
        "            \"CEBPB\"]\n",
        "\n",
        "N_ESTIMATORS = 500\n",
        "RANDOM_STATE = 1\n",
        "N_JOBS = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R3qSVwcC5G7",
        "colab_type": "text"
      },
      "source": [
        "# Example: Training individual TF-classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wSwljs2Nhj3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd60f339-ae9e-4b01-9d35-271bb98389a8"
      },
      "source": [
        "tf_name = TF_NAMES[0]\n",
        "print(tf_name)\n",
        "index = ASBIndex.load(\"asb_data\")\n",
        "feat = index.load_feat_subset(\"TF\") + ['is_asb']\n",
        "feat = list(set(feat) - {'pos', 'ref', 'alt'})\n",
        "ids = index.load_ids_subset(tf_name)\n",
        "df = index.retrieve_datatable(ids, feat, tf_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SPI1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJrRuCywKSmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_rnd_importance = random_feature_importance(RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
        "                                                                      n_jobs=N_JOBS,\n",
        "                                                                      random_state=RANDOM_STATE),\n",
        "                              df.drop(columns=[\"chr\"]),  \n",
        "                              random_state=RANDOM_STATE)\n",
        "\n",
        "df_qual, df_roc, df_pr = cross_validation_chromosomes(RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
        "                                                                      n_jobs=N_JOBS,\n",
        "                                                                      random_state=RANDOM_STATE), \n",
        "                                                      df,\n",
        "                                                      max_rnd_importance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-gejUmyEMDP",
        "colab_type": "text"
      },
      "source": [
        "# Training individual Cellline-classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjCuSTK-EH7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cline_name = CELL_LINE_NAMES[0] \n",
        "print(cline_name)\n",
        "index = ASBIndex.load(\"asb_data\")\n",
        "feat = index.load_feat_subset(\"CL\") + ['is_asb']\n",
        "feat = list(set(feat) - {'pos', 'ref', 'alt'})\n",
        "ids = index.load_ids_subset(cline_name)\n",
        "df = index.retrieve_datatable(ids, feat, cline_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8IprKEfKbdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_rnd_importance = random_feature_importance(RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
        "                                                                      n_jobs=N_JOBS,\n",
        "                                                                      random_state=RANDOM_STATE),\n",
        "                              df.drop(columns=[\"chr\"],  \n",
        "                              random_state=RANDOM_STATE)\n",
        "\n",
        "df_qual, df_roc, df_pr = cross_validation_chromosomes(RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
        "                                                                      n_jobs=N_JOBS,\n",
        "                                                                      random_state=RANDOM_STATE),\n",
        "                                                      df,\n",
        "                                                      max_rnd_importance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqmRa3-5aMUE",
        "colab_type": "text"
      },
      "source": [
        "# Training global TF classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaEP_jhcdCBu",
        "colab_type": "text"
      },
      "source": [
        "Warning: This and next example may require running on server, not Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RumjglXa9A9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_ids = {name: index.get_ids_subset_path(name) for name in TF_NAMES}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovSJnVUz9tj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat = index.load_feat_subset(\"TF\") + ['is_asb']\n",
        "feat = list(set(feat) - {'pos', 'ref', 'alt'})\n",
        "ids = index.base_ids\n",
        "df = index.retrieve_datatable(ids, feat, \"TF_joined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRMk2OU09v74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_rnd_importance = random_feature_importance(RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
        "                                                                      n_jobs=N_JOBS,\n",
        "                                                                      random_state=RANDOM_STATE),\n",
        "                              df.drop(columns=[\"chr\"],  \n",
        "                              random_state=RANDOM_STATE)\n",
        "df_qual, df_roc, df_pr = cross_validation_chromosomes(RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
        "                                                                      n_jobs=N_JOBS,\n",
        "                                                                      random_state=RANDOM_STATE), \n",
        "                                                      df,\n",
        "                                                      max_rnd_importance,\n",
        "                                                      ids_subsets=tf_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47W5vchWawFk",
        "colab_type": "text"
      },
      "source": [
        "## Training global Celline classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxkmdkfF9x3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cl_ids = {name: index.get_ids_subset_path(name) for name in CELL_LINE_NAMES}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_LqO5N-9zzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat = index.load_feat_subset(\"CL\") + ['is_asb']\n",
        "feat = list(set(feat) - {'pos', 'ref', 'alt'})\n",
        "ids = index.base_ids\n",
        "df = index.retrieve_datatable(ids, feat, \"CL_joined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9knuKwKecJwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_rnd_importance = random_feature_importance(RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
        "                                                                      n_jobs=N_JOBS,\n",
        "                                                                      random_state=RANDOM_STATE),\n",
        "                                               df.drop(columns=[\"chr\"],  \n",
        "                                               random_state=1)\n",
        "\n",
        "df_qual, df_roc, df_pr = cross_validation_chromosomes(RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
        "                                                                      n_jobs=N_JOBS,\n",
        "                                                                      random_state=RANDOM_STATE), \n",
        "                                                      df,\n",
        "                                                      max_rnd_importance,\n",
        "                                                      ids_subsets=cl_ids)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}